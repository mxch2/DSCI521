{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module submission header\n",
    "### Submission preparation instructions \n",
    "_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n",
    "\n",
    "### Module submission group\n",
    "- Group member 1\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "- Group member 2\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "- Group member 3\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "- Group member 4\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "\n",
    "### Additional submission comments\n",
    "- Tutoring support received: NA\n",
    "- Other (other): NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment group 2: Network and exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module A _(70 pts)_ Exploring averages, sentiment, and time series\n",
    "In this section we're going to experiment with the word-based text sentiment data generated by a research project documented in the following publication:\n",
    "\n",
    "- https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752#pone.0026752.s001\n",
    "\n",
    "These data are packaged with the assignment in the following directory:\n",
    "\n",
    "- `./data/Data_Set_S1.txt`\n",
    "\n",
    "Here's the caption describing the data, from their paper:\n",
    "\n",
    "> Data from Mechanical Turk study. labMT 1.0 = language assessment by Mechanical Turk 1.0. In the supplementary tab-delimited file named Data Set S1, we provide our set of 10,222 words, their average happiness evaluations according to users on Mechanical Turk, and other information as described below. Please cite the present paper when using this word set. Within papers, we suggest using the abbreviation labMT 1.0 when referencing this data set. The words are ordered according to average happiness (descending), and the file contains eight columns: (1) word, (2) rank, (3) average happiness (50 user evalutions), (4) standard deviation of happiness, (5) Twitter rank, (6) Google Books rank, (7) New York Times rank, (8) Music Lyrics rank. The last four columns correspond to the ranking of a word by frequency of occurrence in the top 5000 words for the specified corpus. A double dash ‘–’ indicates a word was not found in the most frequent 5000 words for a corpus. Please see the main paper for more information regarding this data set.\n",
    "\n",
    "Note, the paper refers to the scorings as 'happiness' values, but these are also referred to as 'valence', which is a measure of poistive/negative 'affect', or 'sentiment'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A1.__ _(2 pts)_ To start, load the dataset with `pandas` into a dataframe and print its `.head()` and `tail()`. Do the lines output make sense in the context of the description?\n",
    "\n",
    "\\[Hint. Since the folks who put these data together put a few lines (3) of descriptive text&mdash;not data&mdash;use the `skiprows` argument in `pd.read_csv()` to start parsing/loading into the dataframe at the appropriate line. Just be sure to keep the header!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A2.__ _(3 pts)_ Review the data with descriptive statistics. In particular, create a histogram of the `'happiness_average'` column, labeling axes clearly, and plot/print out the locations of the mean and median in this same picture as `'red'` and `'blue'` vertical lines respectively. In the response box below indicate if you believe these data are skewed (the center should be `5`, i.e., neutral), and if so indicate the direction of skew (high or low) and how these information justify that view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A3.__ _(3 pts)_ The `'happiness_average'` column was created as an average of 50 survey responses and is supplementally associated to the standard deviations from these 50-respondant cohorts. Make a scatter plot of the `'happiness_standard_deviation'` (vertical) against the `'happiness_average'` (horizontal) column. Again, be sure to clearly label axes and adjust any arguments to make this picture as clear and interpretable as possible.\n",
    "\n",
    "Discuss any relationship you visually observe between these columns in the response box below. Since the `y` variable is a standard deviation, do you think there's a particular range (of the `x`/horizontal variable, `'happiness_average'`) over which the data are specifically less reliable, according to this picture? If so, include some discussion over why you think this may the case, considering the neutral criterion for 'stop word removal' that we follow in part __A10__.\n",
    "\n",
    "\\[Hint. Be sure to utilize the `color` and `alpha` arguments to highlight variations in point density.\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A4.__ _(2 pts)_ One of the columns is labeled `'nyt_rank'`, which indicates the rank (by frequency) of each given word in a corpus of all New York Times (NYT) articles from the 20-year period, 1987&ndash;2007. This is provided because the ~10k words that constitute the data were, in part, drawn from this 20-year corpus of articles. Thus, the words in `./data/Data_Set_S1.txt` data are specially tuned for application to a corpus text drawn from the NYT.\n",
    "\n",
    "So to match, frequencies are provided for the same ~10k words for each daily issue of the NYT in the 20-year corpus. These are contained in the file:\n",
    "\n",
    "- `\"./data/nyt.csv\"`\n",
    "\n",
    "which will available in the data directory after the compressed file is inflated, i.e., after unzipping from the command line in the data directory:\n",
    "\n",
    "```\n",
    ">>> unzip nyt.csv.zip\n",
    "```\n",
    "\n",
    "Load these data using `pandas`, store the `.transpose()`'d dataframe as `nyt`, and then set the dataframe's `.columns` attribute to the `'word'` column of the valence dataframe. Then, print the `.shape`, `'.head()`, and `'.tail()'` to confirm they are compatible with the valence data, e.g., for inner products. In the response box below, record the dimensions $m$ and $n$ as the number of times and words in the dataset (the matrix dimensions). \n",
    "\n",
    "Note: with these data, we're thinking about _time_ as the independent, '`x`' variable, so it's convenient to have the times along the rows, and words along the columns&mdash;this is the 'why' for the application of `.transpose()` from the shape of the `nyt` data on file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A5.__ _(5 pts, total)_ While the valence values were detemined by survey and may be skewed, the histogram from __A2__ doesn't reflect the frequency with which the words were actually used. For this part, your job is to repeate the histogram picture, but with weighted values, including the computation of mean and median lines.\n",
    "\n",
    "So, write a function that computes the mean and median as weighted quantities, according to the occurrence of words in the articles. So intuitively, if the most positive word (`'laughter'`, `happiness_average = 8.5`) appeared 100 times across the NYT, there would be 100 instances of the value 8.5 to incorporate into each of the mean and median calculations.\n",
    "\n",
    "When this is complete, discuss any changing/stationary features of the centrality statistics and histogram that you observe as compared to the output in __A2__, using the response box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._ _(1/5 pts)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] __Weights.__ _(1/5 pts)_\n",
    "To start, total the frequencies in the NYT data across time (collapsing rows) into a single vector: $\\vec{F} = [F_1, \\cdots, F_n]$ (use the `axis` argument in the `.sum()` method and name the resulting object `F`). Using this, record the total number of words in the entire dataset: `N = F.sum()`. Print `N` once calculated, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] __Mean.__ _(1/5 pts)_\n",
    "For the mean you'll have to use $\\vec{F}$ in a sum formula that groups the contributions of words according to their $F_i$ weights:\n",
    "$$\n",
    "\\overline{h} = \\frac{\\sum_{i=1}^nh_i\\cdot F_i}{\\sum_{i=1}^nF_i} = \\frac{1}{N}\\sum_{i=1}^nh_i\\cdot F_i,\n",
    "$$\n",
    "Here, the $h_i$ indicate the valence values for the words, and the denominator $N = \\sum_{i=1}^nF_i$ is equal to the total number of words that appeared in the dataset (as above). Print the mean once calculated, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] __Median.__ _(1/5 pts)_\n",
    "While the above mean can be easily computed  via sums and dot products the 'weighted' median is more challenging. In particular, the median considers the middle-most word according to a sort from low to high by valence. \n",
    "\n",
    "Since we now have frequencies of occurrence for each word, we have to _cumulatively_ count up the frequencies in order of increasing (or decreasing) valence. The median will be the word/valence over which half&mdash;$N/2$&mdash;of the frequency in $\\vec{F}$ accumulates.\n",
    "\n",
    "The valence data are already sorted from high to low. But this is a special case&mdash;we're looking for the 50th percentile. So, we can take cumulative sums of `F` to identify the word for which $N/2$ accumulates. In particular, use the `.cumsum()` method on `F` to identify the median word/valence appearing at $N/2$. Print the median once calculated, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] __Histogram.__ _(1/5 pts)_\n",
    "For the histogram, $\\vec{F}$ can now be used as the value for the `weights` argument. Otherwise, you may reuse your histogram code from __A2__, including vertical lines at the centrality measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A6.__ _(2 pts)_ The NYT data are timeseries data, but how can we utilize the temporal information? To get started, modify `nyt`'s to contain parsed `datetime` objects, utilizing the `dateutil.parser.parse()` function. When this is done, print the first 10 elements of the index.\n",
    "\n",
    "\\[Hint: to get/modify the index in a dataframe utilize the `.index` attribute.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A7.__ _(3 pts)_ Throughout this assignment we're going to want to be able to standardize the columns of our time series. So, your job here is to write a function called `standardize(ts)` that accepts a time series dataframe `ts` (like `nyt`) as input, and outputs a timeseries of same shape, containing the standardized columns of `ts`.\n",
    "\n",
    "For `nyt`, standardizing the columns will allow comparability words, since they occur at very different frequencies. So when `standardize()` processes `nyt`, the $i^\\text{th}$ word at time $t$ should be transformed to: \n",
    "$$\n",
    "\\frac{f_{t,i} - \\mu_T(f_i)}{\\sigma_T(f_i)}\n",
    "$$\n",
    "\n",
    "When this is complete, store the application of `standarize()` to `nyt` as `word_series` and exhibit its `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A8.__ _(10 pts)_ Now write a function called `plot_series(ts, start, stop)` that plots the columns of a time series `ts`, like `word_series` (or `nyt`), as line plots of a range in time specified by strings `start` and `stop` in the format `yyyy-mm-dd`. Be sure to make your visualization as interpretable as possible, specifically utilizing both `label`s for columns and transparency of line color (`alpha`).\n",
    "\n",
    "For development and testing, select just a few word-columns: `word_series[words]` (slicing out columns results in a smaller set of valid series) and a well-understood range of time (`start`/`stop`). In particular, use:\n",
    "\n",
    "- `words = ['new'`, `'year'`, `\"year's\"`, `'two'`, `'eve'`, `'thousand']`\n",
    "- `start = '1999-12-15'`\n",
    "- `stop = '2000-01-15'` \n",
    "\n",
    "for development, and exhibit your code's output using these parameters.\n",
    "\n",
    "When complete, comment in the response box below on the location of peaks, and be sure to discuss why you think the observed spike(s) exist.\n",
    "\n",
    "\\[Hint. to retrieve values in the temporal plotting range defined by `start` and `stop`, use boolean masks.\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A9.__ _(7 pts)_ One common aspect of time series' is the presence of 'uninteresting' trends (at least from the point of view of the desired analysis). Generally, de-trending can be done by characterizing a trend&mdash;a regularity desired for removal&mdash;and  removing it, either by subtraction or division. Right now, our interest will be focused on spikes that may represent events outside of an 'everyday routine', so we will be _de-trending weekly signatures_. \n",
    "\n",
    "So, create a function called `detrend_weekly(ts)` that takes a time series `ts`, computes the average day-of-week values for each column, and the divides column values them by their respective averages from the corresponding days of week. The function should `return` a dataframe the same shape as `ts`.\n",
    "\n",
    "When this is complete, again, apply your code to just a few word-columns: `word_series[words]`. In partcular, use:\n",
    "\n",
    "- `words = ['new'`, `'year'`, `\"year's\"`, `'two'`, `'eve'`, `'thousand']`\n",
    "\n",
    "and pass the result, `detrend_weekly(word_series[words])` through `plot_series()` over the range of time defined by:\n",
    "\n",
    "- `start = '1999-12-15'`\n",
    "- `stop = '2000-01-15'`\n",
    "\n",
    "Finally, comment in the markdown cell below as to any changes that occur when the weekly trend is removed. What do you think these changes tell you about the data, i.e., word frequencies in NYT articles? Do you think the issues for the different days of the week were generally of the same size?\n",
    "\n",
    "\\[Hint: Python support 'triple-index' slicing, for which the third index indicates a step-size for the slice, i.e, for a list `lst`, `lst[0:401:10]` will result in every `10`th value from index `0` to `400`, i.e., `[0, 10, 20, ..., 400]`.\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A10.__ _(8 pts)_ Next up, your job is to produce a function called `avgs(ts, stop = 0)`, which produces a timeseries of average valence values for a times series, `ts`, e.g., the entire 20-year range of `nyt`. \n",
    "\n",
    "In particular, for a given day, $t$, the average daily value should be\n",
    "\n",
    "$$\n",
    "\\overline{h}_t = \\frac{ \\sum_{i=1}^nh_i\\cdot f_{t, i}}{\\sum_{i=1}^nf_{t,i}},\n",
    "$$\n",
    "\n",
    "The argument `stop` indicates the size of a 'stop word window' (see the valence-data authors' paper to supplement details) of valence values for words to be omitted from the calculation. So, if `stop = 1` and the $i^\\text{th}$ word has valence $h_i$ inside the window (i.e., $h_i\\in [4,6]$), then $h_i\\cdot f_{t, i}$ and $f_{t, i}$ should be excluded from the above sum's numerator and denominator, respectively.\n",
    "\n",
    "When this is complete, store the output of this function applied to `nyt` `for` each value of `stop` in `range(4)` as a column in a new time series dataframe called `avgs_series` under the column names `'avg-'+str(stop)`. Then, display these new columns using your `plot_series()` function over a new range, defined by: \n",
    "\n",
    "- `start = \"2000-01-01\"`\n",
    "- `stop = \"2003-12-31\"` \n",
    "\n",
    "and discuss any differences you observe between the four different series defined by the stop word windows in the response box below.\n",
    "\n",
    "\\[Hint: use `.dot()` products and `.sum()` methods on boolean masks of `ts`, slicing the non-stop words for fast calculation.] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A11.__ _(8 pts)_ One thing you might've noticed from the previous part was that the calculated averages were rather noisy, preventing us from observing long-term trends and big events. So, let's build a function called `smooth(ts, r = 1)` that takes a time series `ts` (e.g., `avgs_series`) and returns a same-shaped time series dataframe of _moving averages_ of the columns of your `series`.  \n",
    "\n",
    "Averages are to be taken over the values in a 'window' of radius $r$ about a point in time $t$. In particular, the smoothed value at $t_\\text{smooth}$ should be taken over all times $t$ for which \n",
    "\n",
    "$$t_\\text{smooth} - r < t < t_\\text{smooth} + r$$ \n",
    "\n",
    "For example, if we call the result `smooth_ts = smooth(ts, r)`, then the values should be:\n",
    "\n",
    "```\n",
    "smooth_ts.iloc[i,j] = np.mean(ts[i-r:(i+r)+1, j])\n",
    "```\n",
    "\n",
    "When complete, apply `smooth()` to `avgs_series`, using several values of `r`, and describe how the smoothing affects it in the response box below. To exhibit your code's function, select a value of `r` that you think smoothes the data to an interpretable state, call this `r_best` and then utilize the corresponding output in `plot_series()`, again over the range defined by: \n",
    "\n",
    "- `start = \"2000-01-01\"`\n",
    "- `stop = \"2003-12-31\"`\n",
    "\n",
    "to exemplify your discussion. \n",
    "\n",
    "__Important__: for the edge cases, i.e., when `i < r` or when  `i + r > series.shape[0]`, you'll have to take exta care on indexing to make sure your averages are taken over fewer values. This is the hardest component of this part of the assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A12.__ _(2 pts)_ Now, use `plot_series()` again on the `avgs_series` values, but transform them in the following order:\n",
    "\n",
    "1. `detrend_weekly()`\n",
    "- `standardize()`\n",
    "- `smooth()`\n",
    "\n",
    "and be sure to use your value of `r_best` from the previous part of this assignment.\n",
    "\n",
    "Once complete, take the resulting time series and apply `plot_series()`, again over the range defined by: \n",
    "\n",
    "- `start = \"2000-01-01\"`\n",
    "- `stop = \"2003-12-31\"`\n",
    "\n",
    "Discuss any observations on the effects of standardization and detrending in the response box below, and decide which stop-word window you think produced the 'best' series. Record your 'best' selected series/column from the `avgs_series` dataframe as the string/column name, i.e., `best_column = \"avg-2\"` and extract/store it under the object name `best_series`. You'll be using this selection _only_ throughout the remainder of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A13.__ _(5 pts)_ While smoothing helped us to visually observe some candidate big events on the time series, it will actually make it more difficult to pin down exactly where these shifts happended, and hence provide clues as to _why_ they happened. This is because one means we have for assessing a big 'drop', or, more generally a 'shift', is by looking at outlier _differences_ in the time series values. \n",
    "\n",
    "So, write a new function called `diff(ts)` that calculates _single-step, backward differences_ of `ts` and outputs them as another timeseries dataframe. In particular, this function should subtract the $t-1^\\text{st}$ series values from the $t^\\text{th}$. \n",
    "\n",
    "When this is done, apply your code in the following order to `best_series`:\n",
    "\n",
    "1. `detrend_weekly()`\n",
    "- `standardize()`\n",
    "- `diff()`\n",
    "\n",
    "Then, `.sort(by = best_column)` the result and store it as `diffs`. Finally, print the `.head()` and `.tail()` of the result. Considering the change of sign (negative for `.head()` and positive for `.tail()`), do you recognize any dates that might correspond to sentiment-charged news reporting major geopolitical events?  In the response box below, discuss any of these specific dates and possibilities.\n",
    "\n",
    "__Important__: the output of `diff()` should always be one row smaller than its input, since the first (`0`-index) row has none previous to subtract. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A14.__ _(10 pts)_ To observe candidate events, modify your original `plot_series` function into a new one:\n",
    "\n",
    "- `plot_series_top_diffs(ts, start, stop, ylabel, top = 5, legend = True)` \n",
    "\n",
    "so as to additionally visualize the `top` (most extreme) values determined from an application of `diff()` to `ts`, i.e., the `.head(top)` and `.tail(top)` from the sorted output of `diff()`.  \n",
    "\n",
    "Beyond `plot_series()`, this function should, intuitively, exhibit 'points on a line' over a specified `start`/`stop` range. As in part __C12__, your final product should exhibit the series with transformations in the following order:\n",
    "\n",
    "1. `detrend_weekly()`\n",
    "- `standardize()`\n",
    "- `smooth()`\n",
    "\n",
    "__Important:__ `diff()` should only be applied to _un-smoothed_ data. In other words, for the `2*top` points we'd like to retrieve from `diff` the order of transformations should be:\n",
    "\n",
    "1. `detrend_weekly()`\n",
    "- `standardize()`\n",
    "- `diff()`\n",
    "\n",
    "So, be sure to pass only _detrended_, _standardized_ series to the `plot_series_top_diffs` function. Application of `diff()` and `smooth()` should then be applied to the passed `ts` to identify the `top` times along the `smooth()`'d values for plotting.\n",
    "\n",
    "Also, as with parts __C10__&mdash;__C13__, your final code should be exhibited over the range\n",
    "\n",
    "- `start = \"2000-01-01\"`\n",
    "- `stop = \"2003-12-31\"`\n",
    "\n",
    "but now just in application to the `best_series` values.\n",
    "\n",
    "When this is complete, exhibit the function's output setting `top = 5`. Then, discuss the locations of these points below in the context of the potential events you identified in __C13__. Is the analysis all consistent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
